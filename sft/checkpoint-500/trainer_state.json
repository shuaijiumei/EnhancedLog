{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 4.444444444444445,
  "eval_steps": 500,
  "global_step": 500,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.08888888888888889,
      "grad_norm": 4.975104808807373,
      "learning_rate": 2.9985265915903045e-05,
      "loss": 2.5823,
      "step": 10
    },
    {
      "epoch": 0.17777777777777778,
      "grad_norm": 3.3523364067077637,
      "learning_rate": 2.9932611152294432e-05,
      "loss": 2.2508,
      "step": 20
    },
    {
      "epoch": 0.26666666666666666,
      "grad_norm": 3.4716506004333496,
      "learning_rate": 2.9829462463445186e-05,
      "loss": 1.7261,
      "step": 30
    },
    {
      "epoch": 0.35555555555555557,
      "grad_norm": 2.337277889251709,
      "learning_rate": 2.9679638135302238e-05,
      "loss": 1.4397,
      "step": 40
    },
    {
      "epoch": 0.4444444444444444,
      "grad_norm": 2.332780122756958,
      "learning_rate": 2.948360973899064e-05,
      "loss": 1.2988,
      "step": 50
    },
    {
      "epoch": 0.5333333333333333,
      "grad_norm": 1.814897060394287,
      "learning_rate": 2.9241994272647725e-05,
      "loss": 1.1477,
      "step": 60
    },
    {
      "epoch": 0.6222222222222222,
      "grad_norm": 1.8853600025177002,
      "learning_rate": 2.895555221942537e-05,
      "loss": 1.1094,
      "step": 70
    },
    {
      "epoch": 0.7111111111111111,
      "grad_norm": 2.000004529953003,
      "learning_rate": 2.862518515387408e-05,
      "loss": 1.1084,
      "step": 80
    },
    {
      "epoch": 0.8,
      "grad_norm": 1.6444374322891235,
      "learning_rate": 2.8251932904242803e-05,
      "loss": 1.0664,
      "step": 90
    },
    {
      "epoch": 0.8888888888888888,
      "grad_norm": 1.8741177320480347,
      "learning_rate": 2.7836970279626083e-05,
      "loss": 1.1763,
      "step": 100
    },
    {
      "epoch": 0.9777777777777777,
      "grad_norm": 25.823322296142578,
      "learning_rate": 2.7381603372259895e-05,
      "loss": 1.2512,
      "step": 110
    },
    {
      "epoch": 1.0666666666666667,
      "grad_norm": 2.5658438205718994,
      "learning_rate": 2.688726544660455e-05,
      "loss": 1.2508,
      "step": 120
    },
    {
      "epoch": 1.1555555555555554,
      "grad_norm": 2.390122890472412,
      "learning_rate": 2.6355512428153903e-05,
      "loss": 1.0131,
      "step": 130
    },
    {
      "epoch": 1.2444444444444445,
      "grad_norm": 2.4735865592956543,
      "learning_rate": 2.5788018006169707e-05,
      "loss": 1.0976,
      "step": 140
    },
    {
      "epoch": 1.3333333333333333,
      "grad_norm": 2.1401121616363525,
      "learning_rate": 2.518656836575523e-05,
      "loss": 1.1824,
      "step": 150
    },
    {
      "epoch": 1.4222222222222223,
      "grad_norm": 2.4291481971740723,
      "learning_rate": 2.4553056565848953e-05,
      "loss": 1.1193,
      "step": 160
    },
    {
      "epoch": 1.511111111111111,
      "grad_norm": 2.290972948074341,
      "learning_rate": 2.3889476580833538e-05,
      "loss": 1.1188,
      "step": 170
    },
    {
      "epoch": 1.6,
      "grad_norm": 1.9545772075653076,
      "learning_rate": 2.3197917024514076e-05,
      "loss": 0.9656,
      "step": 180
    },
    {
      "epoch": 1.6888888888888889,
      "grad_norm": 2.1579477787017822,
      "learning_rate": 2.2480554576219313e-05,
      "loss": 1.0514,
      "step": 190
    },
    {
      "epoch": 1.7777777777777777,
      "grad_norm": 2.0689408779144287,
      "learning_rate": 2.173964712971729e-05,
      "loss": 1.0384,
      "step": 200
    },
    {
      "epoch": 1.8666666666666667,
      "grad_norm": 2.229069948196411,
      "learning_rate": 2.0977526686508953e-05,
      "loss": 0.9339,
      "step": 210
    },
    {
      "epoch": 1.9555555555555557,
      "grad_norm": 2.466409921646118,
      "learning_rate": 2.0196592015868187e-05,
      "loss": 0.9538,
      "step": 220
    },
    {
      "epoch": 2.0444444444444443,
      "grad_norm": 2.2193007469177246,
      "learning_rate": 1.93993011047307e-05,
      "loss": 0.8828,
      "step": 230
    },
    {
      "epoch": 2.1333333333333333,
      "grad_norm": 2.032353639602661,
      "learning_rate": 1.8588163421195693e-05,
      "loss": 0.9981,
      "step": 240
    },
    {
      "epoch": 2.2222222222222223,
      "grad_norm": 2.298396587371826,
      "learning_rate": 1.7765732015990914e-05,
      "loss": 0.9417,
      "step": 250
    },
    {
      "epoch": 2.311111111111111,
      "grad_norm": 2.4750301837921143,
      "learning_rate": 1.6934595486761607e-05,
      "loss": 0.9313,
      "step": 260
    },
    {
      "epoch": 2.4,
      "grad_norm": 2.339768648147583,
      "learning_rate": 1.609736983047581e-05,
      "loss": 0.904,
      "step": 270
    },
    {
      "epoch": 2.488888888888889,
      "grad_norm": 2.3316843509674072,
      "learning_rate": 1.5256690209590464e-05,
      "loss": 1.0274,
      "step": 280
    },
    {
      "epoch": 2.5777777777777775,
      "grad_norm": 2.917997121810913,
      "learning_rate": 1.4415202657894233e-05,
      "loss": 0.9445,
      "step": 290
    },
    {
      "epoch": 2.6666666666666665,
      "grad_norm": 2.8950271606445312,
      "learning_rate": 1.3575555752132897e-05,
      "loss": 0.9404,
      "step": 300
    },
    {
      "epoch": 2.7555555555555555,
      "grad_norm": 2.7212517261505127,
      "learning_rate": 1.2740392275630805e-05,
      "loss": 1.1025,
      "step": 310
    },
    {
      "epoch": 2.8444444444444446,
      "grad_norm": 2.6529541015625,
      "learning_rate": 1.1912340900147067e-05,
      "loss": 1.0846,
      "step": 320
    },
    {
      "epoch": 2.9333333333333336,
      "grad_norm": 3.2815661430358887,
      "learning_rate": 1.1094007912147827e-05,
      "loss": 0.9372,
      "step": 330
    },
    {
      "epoch": 3.022222222222222,
      "grad_norm": 3.3106393814086914,
      "learning_rate": 1.0287969009536114e-05,
      "loss": 1.1206,
      "step": 340
    },
    {
      "epoch": 3.111111111111111,
      "grad_norm": 2.479539394378662,
      "learning_rate": 9.496761194658951e-06,
      "loss": 0.9566,
      "step": 350
    },
    {
      "epoch": 3.2,
      "grad_norm": 2.3256711959838867,
      "learning_rate": 8.722874789108555e-06,
      "loss": 0.9001,
      "step": 360
    },
    {
      "epoch": 3.2888888888888888,
      "grad_norm": 2.581721067428589,
      "learning_rate": 7.96874559545085e-06,
      "loss": 0.909,
      "step": 370
    },
    {
      "epoch": 3.3777777777777778,
      "grad_norm": 3.187357187271118,
      "learning_rate": 7.2367472305523046e-06,
      "loss": 0.8911,
      "step": 380
    },
    {
      "epoch": 3.466666666666667,
      "grad_norm": 2.9975547790527344,
      "learning_rate": 6.52918365463588e-06,
      "loss": 0.941,
      "step": 390
    },
    {
      "epoch": 3.5555555555555554,
      "grad_norm": 3.3405818939208984,
      "learning_rate": 5.848281919580787e-06,
      "loss": 0.9917,
      "step": 400
    },
    {
      "epoch": 3.6444444444444444,
      "grad_norm": 2.7002222537994385,
      "learning_rate": 5.196185159290774e-06,
      "loss": 0.8846,
      "step": 410
    },
    {
      "epoch": 3.7333333333333334,
      "grad_norm": 2.538156747817993,
      "learning_rate": 4.574945844193745e-06,
      "loss": 1.0219,
      "step": 420
    },
    {
      "epoch": 3.822222222222222,
      "grad_norm": 2.3477578163146973,
      "learning_rate": 3.986519321104055e-06,
      "loss": 0.9285,
      "step": 430
    },
    {
      "epoch": 3.911111111111111,
      "grad_norm": 2.6391305923461914,
      "learning_rate": 3.4327576587807995e-06,
      "loss": 1.0292,
      "step": 440
    },
    {
      "epoch": 4.0,
      "grad_norm": 2.663729667663574,
      "learning_rate": 2.9154038185530818e-06,
      "loss": 0.828,
      "step": 450
    },
    {
      "epoch": 4.088888888888889,
      "grad_norm": 6.816222190856934,
      "learning_rate": 2.4360861683602737e-06,
      "loss": 0.8599,
      "step": 460
    },
    {
      "epoch": 4.177777777777778,
      "grad_norm": 2.556576728820801,
      "learning_rate": 1.9963133574742405e-06,
      "loss": 0.961,
      "step": 470
    },
    {
      "epoch": 4.266666666666667,
      "grad_norm": 3.043102264404297,
      "learning_rate": 1.5974695680352741e-06,
      "loss": 0.8965,
      "step": 480
    },
    {
      "epoch": 4.355555555555555,
      "grad_norm": 2.025193691253662,
      "learning_rate": 1.240810158347585e-06,
      "loss": 0.8561,
      "step": 490
    },
    {
      "epoch": 4.444444444444445,
      "grad_norm": 5.006158828735352,
      "learning_rate": 9.274577116469601e-07,
      "loss": 0.9235,
      "step": 500
    },
    {
      "epoch": 4.444444444444445,
      "eval_loss": 0.9124124050140381,
      "eval_runtime": 12.838,
      "eval_samples_per_second": 7.789,
      "eval_steps_per_second": 7.789,
      "step": 500
    }
  ],
  "logging_steps": 10,
  "max_steps": 560,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 5,
  "save_steps": 500,
  "total_flos": 7.561587645068083e+16,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
